{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "\n",
    "There are 2 types of summarization:\n",
    "    \n",
    "1. Abstractive summarization\n",
    "\n",
    "1. Extractive summarization.\n",
    "\n",
    "**Abstractive Summarization**: Abstractive methods select words based on semantic understanding, even those words did not appear in the source documents. It aims at producing important material in a new way. They interpret and examine the text using advanced natural language techniques in order to generate a new shorter text that conveys the most critical information from the original text.\n",
    "It can be correlated to the way human reads a text article or blog post and then summarizes in their own word.\n",
    "**Input document → understand context → semantics → create own summary**\n",
    "\n",
    "**Extractive Summarization**: Extractive methods attempt to summarize articles by selecting a subset of words that retain the most important points.\n",
    "This approach weights the important part of sentences and uses the same to form the summary. Different algorithm and techniques are used to define weights for the sentences and further rank them based on importance and similarity among each other.\n",
    "**Input document → sentences similarity → weight sentences → select sentences with higher rank**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert Paragraphs to Sentences\n",
    "2. Text Preprocessing\n",
    "3. Tokenizing the Sentences\n",
    "4. Find Weighted Frequency of Occurrence\n",
    "5. Replace Words by Weighted Frequency in Original Sentences\n",
    "6. Sort Sentences in Descending Order of Sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = open(\"Nabeel Khan-Chief Data Scientist.txt\", 'r',encoding = 'utf-8')\n",
    "file_content = txt_file.read()\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = re.sub('\\[.*?\\]', '', content)\n",
    "content = re.sub('[‘’“”…]', '', content)\n",
    "content = re.sub('\\n', '', content) \n",
    "content = re.sub('\\t\\t', '', content)\n",
    "content = re.sub('[%s]' % re.escape(string.punctuation), '', content)\n",
    "content = re.sub('\\w*\\d\\w*', '', content)\n",
    "content = re.sub('\\W', ' ', content)\n",
    "content = remove_stopwords(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two objects here.............one is the original resume and the other is processed one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Resume Text To Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = nltk.sent_tokenize(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Weighted Frequencies\n",
    "\n",
    "- In order to find the weighted frequencies processed resume is used\n",
    "- First storing all the English stop words from the nltk library into a stopwords variable. \n",
    "- Looping through all the sentences and then corresponding words to first checking if they are stop words. \n",
    "- If not,proceed to check whether the words exist in word_frequency dictionary i.e. word_frequencies, or not. \n",
    "- If the word is encountered for the first time, it is added to the dictionary as a key and its value is set to 1.\n",
    "- If the word previously exists in the dictionary, its value is simply updated by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(content):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the number of occurances of all the words by the frequency of the most occurring word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the scores for each sentence by adding weighted frequencies of the words that occur in that particular sentence\n",
    "- First create an empty sentence_scores dictionary. \n",
    "- The keys of this dictionary will be the sentences and the values will be the corresponding scores of the sentences. \n",
    "\n",
    "- Loop through each sentence in the resume and tokenize the sentence into words.\n",
    "\n",
    "- Check if the word exists in the word_frequencies dictionary. \n",
    "- Calculate the score for only sentences with less than 30 words. \n",
    "- Next, check whether the sentence exists in the sentence_scores dictionary or not. \n",
    "- If the sentence doesn't exist, add it to the sentence_scores dictionary as a key and assign it the weighted frequency of the first word in the sentence, as its value. \n",
    "- On the contrary, if the sentence exists in the dictionary, we simply add the weighted frequency of the word to the existing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "for sent in resume:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sentence_scores dictionary contains sentences with their corresponding score. \n",
    "- To summarize the resume, taking top N sentences with the highest scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "summary_sentences = heapq.nlargest(15, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "summary = re.sub('\\t\\t\\t', ' ', summary)\n",
    "summary = re.sub('\\n', '', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
